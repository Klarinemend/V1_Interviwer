import streamlit as st
from google import genai
from google.genai import types
import os
import json
import time 
import random
from datetime import datetime

# --- CONFIGURA√á√ÉO INICIAL ---
st.set_page_config(page_title="Engenheiro de Requisitos", layout="wide")

# ==============================================================================
# üîê √ÅREA DAS CHAVES DE API (Key Rotation)
# ==============================================================================

# Tenta carregar as chaves do arquivo secreto (.streamlit/secrets.toml)
try:
    api_keys_list = st.secrets["apiKeys"]
except FileNotFoundError:
    st.error("Arquivo .streamlit/secrets.toml n√£o encontrado!")
    st.stop()
except KeyError:
    st.error("A lista 'apiKeys' n√£o foi encontrada dentro do secrets.toml")
    st.stop()

# Fun√ß√£o de sorteio (continua igual)
def get_random_client():
    if not api_keys_list: return None
    selected_key = random.choice(api_keys_list)
    return genai.Client(api_key=selected_key)

# ==============================================================================
# ==============================================================================

MODEL_NAME = "gemini-2.5-flash" # Modelo r√°pido e com contexto gigante
PASTA_HISTORICO = "historico_conversas"
os.makedirs(PASTA_HISTORICO, exist_ok=True)

SYSTEM_INSTRUCTION = """
Voc√™ √© um Engenheiro de Conhecimento S√™nior e consultor em Ontologias.
Seu objetivo √© entrevistar o usu√°rio para modelar um dom√≠nio.
1. Seja polido, curioso e profissional.
2. Use "Escuta Ativa".
3. Se o usu√°rio pedir um RELAT√ìRIO, gere um documento Markdown completo com todas as especifica√ß√µes coletadas.
"""

# --- FUN√á√ïES DE ARQUIVO ---
def salvar_conversa(messages_list):
    if not messages_list:
        st.warning("Nada para salvar.")
        return
    nome_arquivo = datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".json"
    caminho = os.path.join(PASTA_HISTORICO, nome_arquivo)
    with open(caminho, "w", encoding="utf-8") as f:
        json.dump(messages_list, f, ensure_ascii=False, indent=4)
    st.sidebar.success(f"Salvo: {nome_arquivo}")

def carregar_conversa(nome_arquivo):
    caminho = os.path.join(PASTA_HISTORICO, nome_arquivo)
    try:
        with open(caminho, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

def listar_conversas_salvas():
    try:
        arquivos = [f for f in os.listdir(PASTA_HISTORICO) if f.endswith('.json')]
        arquivos.sort(reverse=True)
        return arquivos
    except:
        return []

# --- ESTADO (SESSION STATE) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "timestamps_msgs" not in st.session_state:
    st.session_state.timestamps_msgs = []

# --- BARRA LATERAL ---
with st.sidebar:
    st.title("üìÇ Menu")
    
    if st.button("‚ûï Nova Conversa", use_container_width=True):
        st.session_state.messages = [] 
        st.session_state.timestamps_msgs = [] 
        st.rerun()

    if st.button("üíæ Salvar Hist√≥rico", use_container_width=True):
        salvar_conversa(st.session_state.messages)
        st.rerun()
       
    st.subheader("Hist√≥rico Salvo")
    arquivos_disponiveis = listar_conversas_salvas()
    
    if arquivos_disponiveis:
        # Caixa de sele√ß√£o
        arquivo_selecionado = st.selectbox("Selecione o arquivo:", arquivos_disponiveis)
        
        # Cria duas colunas para os bot√µes ficarem lado a lado
        col_carregar, col_deletar = st.columns(2)
        
        # Bot√£o CARREGAR
        with col_carregar:
            if st.button("üìÇ Abrir", use_container_width=True):
                historico_recuperado = carregar_conversa(arquivo_selecionado)
                if historico_recuperado:
                    st.session_state.messages = historico_recuperado
                    st.session_state.timestamps_msgs = []
                    st.success("Carregado!")
                    time.sleep(0.5) # D√° tempo de ler a mensagem
                    st.rerun()

        # Bot√£o DELETAR
        with col_deletar:
            # type="primary" deixa o bot√£o com destaque (geralmente vermelho/colorido dependendo do tema)
            if st.button("üóëÔ∏è Excluir", type="primary", use_container_width=True):
                caminho_completo = os.path.join(PASTA_HISTORICO, arquivo_selecionado)
                try:
                    os.remove(caminho_completo)
                    st.toast(f"Arquivo deletado: {arquivo_selecionado}")
                    time.sleep(0.5)
                    st.rerun() # Recarrega a p√°gina para atualizar a lista
                except Exception as e:
                    st.error(f"Erro ao deletar: {e}")

    else:
        st.caption("Nenhuma conversa salva ainda.")


# --- CHAT PRINCIPAL ---
st.title("ü§ñ Engenheiro de Requisitos")
st.caption("Sou um consultor especializado, irei te entrevistar para entender e modelar as regras do seu sistema/dom√≠nio.\n Poderia me falar sobre ele?")

# Exibe todo o hist√≥rico visualmente
for msg in st.session_state.messages:
    role = "user" if msg["role"] == "user" else "assistant"
    avatar = "üßë‚Äçüíª" if role == "user" else "ü§ñ"
    with st.chat_message(role, avatar=avatar):
        st.markdown(msg["content"])

# --- L√ìGICA DE ENVIO ---
prompt = st.chat_input("Digite aqui...")

if prompt:
    # 1. Mostra e salva a mensagem do usu√°rio
    with st.chat_message("user", avatar="üßë‚Äçüíª"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # 2. Processamento da IA
    with st.chat_message("assistant", avatar="ü§ñ"):
        placeholder = st.empty()
        placeholder.markdown("Pensando...")
        
        try:
            # === L√ìGICA INTELIGENTE DE CONTEXTO ===
            
            # Detecta se √© um pedido de relat√≥rio final
            termos_relatorio = ["gerar relatorio", "gerar relat√≥rio", "relat√≥rio final", "resumo completo", "documenta√ß√£o"]
            eh_relatorio = any(t in prompt.lower() for t in termos_relatorio)
            
            msgs_para_contexto = []
            
            if eh_relatorio:
                # MODO RELAT√ìRIO: Envia TUDO (Hist√≥rico Completo)
                # O Gemini Flash tem 1 milh√£o de tokens, aguenta ler tudo de uma vez.
                st.toast("üìÑ Detectado pedido de relat√≥rio: Lendo hist√≥rico completo...", icon="üß†")
                msgs_para_contexto = st.session_state.messages[:-1] # Tudo at√© antes do prompt atual
            else:
                # MODO CONVERSA: Janela Deslizante (√öltimas 10 mensagens)
                # Mant√©m o custo baixo e a velocidade alta.
                JANELA = 10
                if len(st.session_state.messages) > JANELA:
                    msgs_para_contexto = st.session_state.messages[-(JANELA+1):-1]
                else:
                    msgs_para_contexto = st.session_state.messages[:-1]
            
            # Converte para formato da API
            api_history = []
            for msg in msgs_para_contexto:
                r = "user" if msg["role"] == "user" else "model"
                api_history.append(types.Content(
                    role=r,
                    parts=[types.Part(text=msg["content"])]      
                ))
            
            # === ROTA√á√ÉO DE CHAVES E ENVIO ===
            client = get_random_client()
            if not client:
                raise Exception("Configure as chaves de API no c√≥digo!")

            # Cria chat tempor√°rio configurado com o hist√≥rico escolhido
            chat = client.chats.create(
                model=MODEL_NAME,
                config=types.GenerateContentConfig(
                    system_instruction=SYSTEM_INSTRUCTION,
                    temperature=0.7
                ),
                history=api_history
            )
            
            # Envia a mensagem atual
            response = chat.send_message(prompt)
            texto = response.text
            
            # Exibe e salva
            placeholder.markdown(texto)
            st.session_state.messages.append({"role": "model", "content": texto})
            
            # Atualiza contador de RPM
            st.session_state.timestamps_msgs.append(time.time())
            time.sleep(0.1) # Pequena pausa para estabilidade
            st.rerun()

        except Exception as e:
            st.error(f"Erro: {e}")
